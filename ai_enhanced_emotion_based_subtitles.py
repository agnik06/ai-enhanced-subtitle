# -*- coding: utf-8 -*-
"""AI enhanced emotion based subtitles.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1psnn_O2_MrZYbZkXVk41FpZGH2ywbSou
"""

import cv2
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report
from moviepy.editor import VideoFileClip, AudioFileClip

# Sample subtitle data (for demonstration purposes)
subtitle_events = [
    {"text": "Let me begin this evening by expressing my gratitude...", "start": 0.0, "duration": 7.2, "emotion": "joy"},
    {"text": "Of love and support following the assassination attempt at my rally on Saturday", "start": 7.24, "duration": 6.18, "emotion": "sadness"},
    {"text": "As you already know, the bullet came within a quarter of an inch of taking my", "start": 19.02, "duration": 7.48, "emotion": "surprise"},
    {"text": "life. So many people have asked me what happened.", "start": 26.5, "duration": 3.64, "emotion": "neutral"},
]

# Prepare training data from subtitle_events
texts, labels = zip(*[(event["text"], event["emotion"]) for event in subtitle_events])

# Convert text data into TF-IDF features
vectorizer = TfidfVectorizer()
X_train = vectorizer.fit_transform(texts)

# Train a basic classifier (Naive Bayes)
classifier = MultinomialNB()
classifier.fit(X_train, labels)

# Function to classify emotion using the trained classifier
def classify_emotion(text):
    X_text = vectorizer.transform([text])
    predicted_emotion = classifier.predict(X_text)[0]
    return predicted_emotion

# Function to add subtitles with a typing effect and color based on emotion
def add_subtitle_with_typing_effect(frame, text, position, current_time, start_time, duration, emotion, font_scale=0.7, thickness=2, font=cv2.FONT_HERSHEY_COMPLEX):
    elapsed_time = current_time - start_time
    text_length = len(text)
    if elapsed_time >= 0:
        chars_to_show = min(int((elapsed_time / duration) * text_length), text_length)
        partial_text = text[:chars_to_show]
    else:
        partial_text = ""

    text_size = cv2.getTextSize(partial_text, font, font_scale, thickness)[0]
    text_x = position[0] - text_size[0] // 2
    text_y = position[1]

    # Emotion-based color selection
    color_map = {
        "joy": (0, 255, 0),  # Green
        "sadness": (255, 0, 0),  # Blue
        "anger": (0, 0, 255),  # Red
        "neutral": (255, 255, 255),  # White
        "surprise": (255, 255, 0)  # Yellow
    }
    text_color = color_map.get(emotion, (255, 255, 255))  # Default to white

    cv2.rectangle(frame, (text_x - 10, text_y - text_size[1] - 10), (text_x + text_size[0] + 10, text_y + 10), (0, 0, 0), -1)
    cv2.putText(frame, partial_text, (text_x, text_y), font, font_scale, text_color, thickness, cv2.LINE_AA)
    return frame

# Classify emotions for each subtitle event (using classifier)
classified_events = []
for event in subtitle_events:
    emotion = classify_emotion(event['text'])
    classified_events.append({**event, "emotion": emotion})

# Load the video file
video_path = "/content/samplevideo.mp4"
cap = cv2.VideoCapture(video_path)

# Get video properties
fps = int(cap.get(cv2.CAP_PROP_FPS))
width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

# Create a VideoWriter to save the output
fourcc = cv2.VideoWriter_fourcc(*'XVID')
out = cv2.VideoWriter("output_with_typing_subtitles.avi", fourcc, fps, (width, height))

# Process each frame
frame_index = 0
while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    current_time = frame_index / fps  # Calculate the current time in seconds

    # Check if there's a subtitle to show in this frame
    for event in classified_events:
        if event["start"] <= current_time < event["start"] + event["duration"]:
            frame = add_subtitle_with_typing_effect(
                frame, event["text"], (width // 2, height - 30), current_time, event["start"], event["duration"], event["emotion"]
            )
            break

    out.write(frame)  # Write the frame with the subtitle
    frame_index += 1

# Release the video objects
cap.release()
out.release()

# Add audio using MoviePy
video_clip = VideoFileClip("output_with_typing_subtitles.avi")
audio_clip = AudioFileClip("/content/samplevideo.mp4")  # Replace with your audio file path
final_video = video_clip.set_audio(audio_clip)

# Save the final video with audio
final_video.write_videofile("final_output_with_emotion_typing_effect.mp4", codec="libx264")

# Generate classification report for the emotions
true_labels = [event['emotion'] for event in classified_events]
pred_labels = [classify_emotion(event['text']) for event in subtitle_events]
print("Classification Report:\n", classification_report(true_labels, pred_labels))

print("Final video with typing effect, emotion-based subtitles, and audio saved as 'final_output_with_emotion_typing_effect.mp4'")